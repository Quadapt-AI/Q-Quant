// This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/
// Â© QuadaptTrader
// Author: Kenneth E.
//@version=5
library("MLClassificationFunctions", true)


//-----------------------------------------------------------------------------{
// Logistic Regression
//-----------------------------------------------------------------------------{

// Logistic regression is a statistical modeling technique used for binary classification and probability estimation tasks. It's a type of regression analysis that's 
// particularly well-suited for scenarios where the dependent variable is binary (e.g., 0 or 1, Yes or No) or categorical with two classes.
//
//Key characteristics and concepts of logistic regression include:
    // [1] Sigmoid Function: Logistic regression uses the logistic function (sigmoid function) to model the relationship between the independent variables and 
    //     the log-odds of the dependent variable. The sigmoid function maps any real-valued number into a range between 0 and 1, which can be interpreted as probabilities.
    // [2] Log-Odds: Logistic regression models the log-odds (logit) of the probability of the positive class. The log-odds are linearly related to the independent variables through a weighted sum.
    // [3] Binary Classification: Logistic regression is commonly used for binary classification tasks, where the outcome variable can take one of two possible classes. 
    //     For example, it can be used for spam vs. non-spam email classification, disease diagnosis (presence or absence), and more.
    // [4] Estimating Probabilities: Logistic regression can provide not only binary predictions but also estimates of the probability that an instance belongs to a 
    //     particular class. These probabilities can be used to make informed decisions and set classification thresholds.
    // [5] Model Coefficients: The logistic regression model involves coefficients (weights) for each independent variable, which are estimated during model training. 
    //     These coefficients influence the impact of each independent variable on the log-odds of the outcome.
    // [6] Maximum Likelihood Estimation: The model parameters (coefficients) are estimated using maximum likelihood estimation (MLE), a statistical technique that 
    //     finds the values that maximize the likelihood of the observed data given the model.
    // [7] Evaluation Metrics: Common evaluation metrics for logistic regression include accuracy, precision, recall, F1-score, ROC curve, and AUC (Area Under the Curve).
    // [8] Regularization: To prevent overfitting, logistic regression models can be regularized using techniques like L1 regularization (Lasso) or L2 regularization (Ridge).
    // [9] Multinomial Logistic Regression: While binary logistic regression deals with two classes, multinomial logistic regression extends the technique to handle more than two classes.

// Logistic regression is widely used in various fields, including healthcare (disease prediction), marketing (customer churn analysis), finance (credit risk modeling), and many 
// other applications where binary classification or probability estimation is essential. It's a fundamental tool in the machine learning and statistical analysis toolkit.

//-----------------------------------------------------------------------------{
// Utils
//-----------------------------------------------------------------------------{

export dot(series float v, series float w, series int p) =>  // dot product
    // \sum_{i = 1}^{p} v \dot w
    // @function dot : Returns the inner product of two vectors
    // @params: <float v> : First vector
    // @params: <float w> : Secod vector
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : The inner product of two vectors
    math.sum(v * w, p)

export minimax(series float value, series int p, series float min,series float max) =>
    // (max - min) * (value - low) / (high - minimum) + minimum
    // @function Price normalization : Returns normalization value of the prediction
    // @params <int value> : input value to normalize
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float min> : min value over range of p-values (not probabilities)
    // @params: <float max> : max value over range of p-values (not probabilities)
    // @returns <float float> : minimax value
    hi = ta.highest(value, p)
    lo = ta.lowest(value, p)
    (max - min) * (value - lo) / (hi - lo) + min

export reqularizer(string r, series float w, series int p, series float alpha, series float lambda) =>
    // @function Regularization term : Returns regularization quantity to control overfitting and underfitting
    // @params: <float w> : Secod vector
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float alpha> : Hyperparameter (inner) I
    // @params: <float lambda> : Hyperparameter (outer) II
    // @returns <float float> : L1, L2 or L1-L2 (Elastic Net) regularization
    switch r
        'l1' => lambda * math.abs(w)
        'l2' => lambda * dot(w, w, p)
        'enet' => lambda * dot(alpha * dot(w, w, p), (1 - alpha) * math.abs(w), p)
        'nr' => 0.0 //No regularization


export sigmoid(series float z) => 
    // 1.0 / (1.0 + exp(-z))
    // @function Sigmoid : Returns the output of a sigmoid function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    1.0 / (1.0 + math.exp(-z))

// Cost function of Logistic Regression
export cost_logit(series float Y, series float y_hat, series int p) =>
    // -1.0 / p \sum_{i = 1}^{p} y_i ln y_{hat} + (1 - y_i) ln (1 - y_hat)
    // @function loss function of Logit : Returns the loss function of Logistic regression model
    // @params: <float Y> : input data of the label to classify
    // @params: <float y_hat> :The hypothesis of the classification on Y
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : cost (loss) output
    1.0 / p * dot(dot(Y, math.log(y_hat), p), dot((1.0 - Y), math.log(1 - y_hat), p), p)

//-----------------------------------------------------------------------------{
// Training
//-----------------------------------------------------------------------------{

//------ Training Regularized Logistic Regression with Elastic Net
export logistic_regression(series float X, series float Y, series int p, series float lr, series float _lambda, series float _alpha, series float epochs, series string r) =>
    // @function Training function : Returns the training loss and hypothesis of the Elastic Net logistic regression model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float _lambda> : Weight parameter fixed on the regularizer e.g 0.1
    // @params: <float _alpha> : Weight parameter fixed on the regularizer e.g 0.5
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for Elastic Net regularized logistic regression
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = sigmoid(dot(X, 0.0, p))  //-- prediction
        loss := cost_logit(Y, y_hat, p) + reqularizer(r, w, p, _alpha, _lambda)
        gradient = 1.0 / p * dot(X, y_hat - Y, p)
        w -= lr * gradient  //-- update weights
        w
    [loss, sigmoid(dot(X, w, p))]  //-- current loss & prediction

//-----------------------------------------------------------------------------{
// Support Vector Machine
//-----------------------------------------------------------------------------{

//-----------------------------------------------------------------------------{
// Utils
//-----------------------------------------------------------------------------{

//------ Margin 
export margin(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the SVM margin or hypothesis
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : Margin
    Y * (dot(X, w, p) + b)

//--SVM hinge loss
export hinge(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the the SVM hinge loss
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : hinge loss
    math.max(0, 1 - margin(X, Y, w, b, p))

//----- Cost function
export cost_svm(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the training loss and hypothesis of SVM
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : cost (total loss) output
    float C = 0.01
    1.0 / p * 0.5 * dot(w, w, p) + C * math.sum(hinge(X, Y, w, b, p), p)

//-- Output axon
export svm_axon(series float z) =>
    // @function Sigmoid : Returns the output of the SVM function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    math.sign(z)

//-----------------------------------------------------------------------------{
// Training
//-----------------------------------------------------------------------------{

//--- Training Linear SVM
export linear_SVM(series float X, series float Y, series int p, series float lr, series float C, series float _lambda, series float _alpha, series float epochs, series string r) =>
    // w -> weight; loss -> SVM loss; b -> bias weight C -> relaxation term
    // @function Training function : Returns the training loss and hypothesis of linear Support Vector Machine (SVM)
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float C> : Weight parameter fixed and tunable hyperparameter e.g 0.1
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for SVM
    float w = 0.0
    float loss = 0.0
    float b = 0.0
    for i = 1 to epochs
        y_hat = margin(X, Y, 0.0, 0.0, p)
        if y_hat < 1
            loss := cost_svm(X, Y, w, b, p) + reqularizer(r, w, p, _alpha, _lambda)
            gradient = 1.0 / p * (b - C * dot(Y, X, p))
            b := 1.0 / p * (b - lr * C * math.sum(Y, p))
            w -= lr * gradient
            w
    [loss, svm_axon(dot(X, w, p) + b)]

//-----------------------------------------------------------------------------{
// Perceptron
//-----------------------------------------------------------------------------{

//-----------------------------------------------------------------------------{
// Utils
//-----------------------------------------------------------------------------{

//------------ Prediction
export tanh(series float X, series float w, series int p) =>
    // @function tanh : Returns the hyperbolic tangent (tanh) of a vector
    // @params: <float X> : Input predictor data
    // @params: <float w> : training weight
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : Returns the hyperbolic tangent (tanh)
    (math.exp(dot(X, w, p)) - math.exp(-dot(X, w, p))) / (math.exp(dot(X, w, p)) + math.exp(-dot(X, w, p)))

//------------- Cost function
export cost_perceptron(series float Y, series float y_hat) =>
    // @function Training cost function : Returns the training loss of a perceptron
    // @params: <float Y> : Input data of the label to classify
    // @params: <float y_hat> :The hypothesis of the classification on Y
    // @returns <float float> : cost (loss) output for Perceptron
    Y - y_hat

//------------- Activation function
export activation_perceptron(series float z) =>
    // @function Perceptron activation : Returns the output of a perceptron function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    result = z > 0 ? 1 : 0
    result

//-----------------------------------------------------------------------------{
// Training
//-----------------------------------------------------------------------------{

//------- Training a perceptron
export perceptron(series float X, series float Y, series int p, series float _lambda, series float _alpha, series float epochs, series string r) =>
    // @function Training function : Returns the training loss and hypothesis of Perceptron model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = tanh(X, w, p)
        if Y != y_hat
            loss := cost_perceptron(Y, y_hat) + reqularizer(r, w, p, _alpha, _lambda)
            w += 0.1 * dot(loss, X, p)
            w
    [loss, activation_perceptron(dot(X, w, p))]


// Example
reg = 'nr'
p = 50
nm = 5
alpha = 0.1
lambda = 0.5
//-logit
[loss_lgit, y_hat_lgit] = logistic_regression(close, math.log(hl2), p, 1e-3, lambda, alpha, 1000, reg)
y_hat_scaled_lgit = minimax(y_hat_lgit, nm, ta.lowest(close, nm), ta.highest(close, nm))
//-SVM
[loss_svm, y_hat_svm] = linear_SVM(close, hl2, p, 1e-3, 0.1, lambda, alpha, 1000, reg)
y_hat_scaled_svm = minimax(y_hat_svm, nm, ta.lowest(close, nm), ta.highest(close, nm))
//-Perceptron
[loss_p, y_hat_p] = perceptron(close, hl2, p, lambda, alpha, 1000, reg)
y_hat_scaled_p = minimax(y_hat_p, nm, ta.lowest(close, nm), ta.highest(close, nm))
//-Render plots
plot(y_hat_scaled_lgit, color = color.white, title = "Logit")
plot(y_hat_scaled_svm, color = color.red, title = "SVM")
plot(y_hat_scaled_p, color = color.blue, title = "Perceptron")


