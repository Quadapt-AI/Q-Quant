// This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/
// Â© QuadaptTrader
// Author: Kenneth E.

//@version=5
library("MLClassificationFunctions", true)


//-----------------------------------------------------------------------------{
// Logistic Regression
//-----------------------------------------------------------------------------{

// Logistic regression is a statistical modeling technique used for binary classification and probability estimation tasks. It's a type of regression analysis that's 
// particularly well-suited for scenarios where the dependent variable is binary (e.g., 0 or 1, Yes or No) or categorical with two classes.
//
//Key characteristics and concepts of logistic regression include:
    // [1] Sigmoid Function: Logistic regression uses the logistic function (sigmoid function) to model the relationship between the independent variables and 
    //     the log-odds of the dependent variable. The sigmoid function maps any real-valued number into a range between 0 and 1, which can be interpreted as probabilities.
    // [2] Log-Odds: Logistic regression models the log-odds (logit) of the probability of the positive class. The log-odds are linearly related to the independent variables through a weighted sum.
    // [3] Binary Classification: Logistic regression is commonly used for binary classification tasks, where the outcome variable can take one of two possible classes. 
    //     For example, it can be used for spam vs. non-spam email classification, disease diagnosis (presence or absence), and more.
    // [4] Estimating Probabilities: Logistic regression can provide not only binary predictions but also estimates of the probability that an instance belongs to a 
    //     particular class. These probabilities can be used to make informed decisions and set classification thresholds.
    // [5] Model Coefficients: The logistic regression model involves coefficients (weights) for each independent variable, which are estimated during model training. 
    //     These coefficients influence the impact of each independent variable on the log-odds of the outcome.
    // [6] Maximum Likelihood Estimation: The model parameters (coefficients) are estimated using maximum likelihood estimation (MLE), a statistical technique that 
    //     finds the values that maximize the likelihood of the observed data given the model.
    // [7] Evaluation Metrics: Common evaluation metrics for logistic regression include accuracy, precision, recall, F1-score, ROC curve, and AUC (Area Under the Curve).
    // [8] Regularization: To prevent overfitting, logistic regression models can be regularized using techniques like L1 regularization (Lasso) or L2 regularization (Ridge).
    // [9] Multinomial Logistic Regression: While binary logistic regression deals with two classes, multinomial logistic regression extends the technique to handle more than two classes.

// Logistic regression is widely used in various fields, including healthcare (disease prediction), marketing (customer churn analysis), finance (credit risk modeling), and many 
// other applications where binary classification or probability estimation is essential. It's a fundamental tool in the machine learning and statistical analysis toolkit.

export dot(series float v, series float w, series int p) =>  // dot product
    // @function dot : Returns the inner product of two vectors
    // @params: <float v> : First vector
    // @params: <float w> : Secod vector
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : The inner product of two vectors
    math.sum(v * w, p)

export minimax(series float ds, series int p, series float min,series float max) =>  // normalize to price
    hi = ta.highest(ds, p)
    lo = ta.lowest(ds, p)
    (max - min) * (ds - lo) / (hi - lo) + min

export sigmoid(series float z) =>  //activation function
    // @function Sigmoid : Returns the output of a sigmoid function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    1.0 / (1.0 + math.exp(-z))

// Cost function of Logistic Regression
export cost_logit(series float Y, series float y_hat, series int p) =>
    // @function loss function of Logit : Returns the loss function of Logistic regression model
    // @params: <float Y> : input data of the label to classify
    // @params: <float y_hat> :The hypothesis of the classification on Y
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : cost (loss) output
    -1.0 / p * dot(dot(Y, math.log(y_hat) + 1.0 - Y, p), math.log(1.0 - y_hat), p)

//------ Training Logistic Regression
export logistic_regression(series float X, series float Y, series int p, series float lr, series float epochs) =>
    // @function Training function : Returns the training loss and hypothesis of the logistic regression model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = sigmoid(dot(X, 0.0, p))  //-- prediction
        loss := cost_logit(Y, y_hat, p)
        gradient = 1.0 / p * dot(X, y_hat - Y, p)
        w -= lr * gradient  //-- update weights
        w
    [loss, sigmoid(dot(X, w, p))]  //-- current loss & prediction

//------ Training Regularized Logistic Regression with L1-norm
export logistic_regression_l1(series float X, series float Y, series int p, series float lr, series float _lambda, series float epochs) =>
    // @function Training function : Returns the training loss and hypothesis of the L1-norm logistic regression model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float _lambda> : Weight parameter fixed on the regularizer e.g 0.1
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for L1-norm regularized logistic regression
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = sigmoid(dot(X, 0.0, p))  //-- prediction
        loss := cost_logit(Y, y_hat, p) + _lambda * math.abs(w)
        gradient = 1.0 / p * dot(X, y_hat - Y, p)
        w -= lr * gradient  //-- update weights
        w
    [loss, sigmoid(dot(X, w, p))]  //-- current loss & prediction

//------ Training Regularized Logistic Regression with L2-norm
export logistic_regression_l2(series float X, series float Y, series int p, series float lr, series float _lambda, series float epochs) =>
    // @function Training function : Returns the training loss and hypothesis of the L2-norm logistic regression model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float _lambda> : Weight parameter fixed on the regularizer e.g 0.1
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for L2-norm regularized logistic regression
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = sigmoid(dot(X, 0.0, p))  //-- prediction
        loss := cost_logit(Y, y_hat, p) + _lambda * dot(w, w, p)
        gradient = 1.0 / p * dot(X, y_hat - Y, p)
        w -= lr * gradient  //-- update weights
        w
    [loss, sigmoid(dot(X, w, p))]  //-- current loss & prediction

//------ Training Regularized Logistic Regression with Elastic Net
export logistic_regression_enet(series float X, series float Y, series int p, series float lr, series float _lambda, series float _alpha, series float epochs) =>
    // @function Training function : Returns the training loss and hypothesis of the Elastic Net logistic regression model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float _lambda> : Weight parameter fixed on the regularizer e.g 0.1
    // @params: <float _alpha> : Weight parameter fixed on the regularizer e.g 0.5
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for Elastic Net regularized logistic regression
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = sigmoid(dot(X, 0.0, p))  //-- prediction
        loss := cost_logit(Y, y_hat, p) + _lambda * dot(_alpha * dot(w, w, p), (1 - _alpha) * math.abs(w), p)
        gradient = 1.0 / p * dot(X, y_hat - Y, p)
        w -= lr * gradient  //-- update weights
        w
    [loss, sigmoid(dot(X, w, p))]  //-- current loss & prediction

//-----------------------------------------------------------------------------{
// Support Vector Machine
//-----------------------------------------------------------------------------{

//------ Margin 
export margin(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the SVM margin or hypothesis
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : Margin
    Y * (dot(X, w, p) + b)

//--SVM hinge loss
export hinge(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the the SVM hinge loss
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : hinge loss
    math.max(0, 1 - margin(X, Y, w, b, p))

//----- Cost function
export cost_svm(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the training loss and hypothesis of SVM
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : cost (total loss) output
    float C = 0.01
    1.0 / p * 0.5 * dot(w, w, p) + C * math.sum(hinge(X, Y, w, b, p), p)

//-- Output axon
export svm_axon(series float z) =>
    // @function Sigmoid : Returns the output of the SVM function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    math.sign(z)

//--- Training Linear SVM
export linear_SVM(series float X, series float Y, series int p, series float lr, series float C, series float epochs) =>
    // w -> weight; loss -> SVM loss; b -> bias weight C -> relaxation term
    // @function Training function : Returns the training loss and hypothesis of linear Support Vector Machine (SVM)
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float C> : Weight parameter fixed and tunable hyperparameter e.g 0.1
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for SVM
    float w = 0.0
    float loss = 0.0
    float b = 0.0
    for i = 1 to epochs
        y_hat = margin(X, Y, 0.0, 0.0, p)
        if y_hat < 1
            loss := cost_svm(X, Y, w, b, p)
            gradient = 1.0 / p * (b - C * dot(Y, X, p))
            b := 1.0 / p * (b - lr * C * math.sum(Y, p))
            w -= lr * gradient
            w
    [loss, svm_axon(dot(X, w, p) + b)]

//-----------------------------------------------------------------------------{
// Perceptron
//-----------------------------------------------------------------------------{

//------------ Prediction
export tanh(series float X, series float w, series int p) =>
    // @function tanh : Returns the hyperbolic tangent (tanh) of a vector
    // @params: <float X> : Input predictor data
    // @params: <float w> : training weight
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : Returns the hyperbolic tangent (tanh)
    (math.exp(dot(X, w, p)) - math.exp(-dot(X, w, p))) / (math.exp(dot(X, w, p)) + math.exp(-dot(X, w, p)))

//------------- Cost function
export cost_perceptron(series float Y, series float y_hat) =>
    // @function Training cost function : Returns the training loss of a perceptron
    // @params: <float Y> : Input data of the label to classify
    // @params: <float y_hat> :The hypothesis of the classification on Y
    // @returns <float float> : cost (loss) output for Perceptron
    Y - y_hat

//------------- Activation function
export activation_perceptron(series float z) =>
    // @function Perceptron activation : Returns the output of a perceptron function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    float result = 0.
    if math.sign(z) > 0
        result := 1
        result
    else if math.sign(z) < 0
        result := 0
        result
    result

//------- Training a perceptron
export perceptron(series float X, series float Y, series int p, series float epochs) =>
    // @function Training function : Returns the training loss and hypothesis of Perceptron model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = tanh(X, w, p)
        if Y != y_hat
            loss := cost_perceptron(Y, y_hat)
            w += 0.1 * dot(loss, X, p)
            w
    [loss, activation_perceptron(dot(X, w, p))]


// Example
[loss, y_hat] = logistic_regression(close, close, 100, 1e-3, 1000)
y_hat_scaled = minimax(y_hat, 30, ta.lowest(close, 30), ta.highest(close, 30))

plot(y_hat_scaled, color = color.white, title = "Prediction")


