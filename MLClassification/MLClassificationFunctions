// This source code is subject to the terms of the Mozilla Public License 2.0 at https://mozilla.org/MPL/2.0/
// Â© QuadaptTrader
// Author: Kenneth E.
//@version=5
library("MLClassificationFunctions", true)


//-----------------------------------------------------------------------------{
// Logistic Regression
//-----------------------------------------------------------------------------{

// Logistic regression is a statistical modeling technique used for binary classification and probability estimation tasks. It's a type of regression analysis that's 
// particularly well-suited for scenarios where the dependent variable is binary (e.g., 0 or 1, Yes or No) or categorical with two classes.
//
//Key characteristics and concepts of logistic regression include:
    // [1] Sigmoid Function: Logistic regression uses the logistic function (sigmoid function) to model the relationship between the independent variables and 
    //     the log-odds of the dependent variable. The sigmoid function maps any real-valued number into a range between 0 and 1, which can be interpreted as probabilities.
    // [2] Log-Odds: Logistic regression models the log-odds (logit) of the probability of the positive class. The log-odds are linearly related to the independent variables through a weighted sum.
    // [3] Binary Classification: Logistic regression is commonly used for binary classification tasks, where the outcome variable can take one of two possible classes. 
    //     For example, it can be used for spam vs. non-spam email classification, disease diagnosis (presence or absence), and more.
    // [4] Estimating Probabilities: Logistic regression can provide not only binary predictions but also estimates of the probability that an instance belongs to a 
    //     particular class. These probabilities can be used to make informed decisions and set classification thresholds.
    // [5] Model Coefficients: The logistic regression model involves coefficients (weights) for each independent variable, which are estimated during model training. 
    //     These coefficients influence the impact of each independent variable on the log-odds of the outcome.
    // [6] Maximum Likelihood Estimation: The model parameters (coefficients) are estimated using maximum likelihood estimation (MLE), a statistical technique that 
    //     finds the values that maximize the likelihood of the observed data given the model.
    // [7] Evaluation Metrics: Common evaluation metrics for logistic regression include accuracy, precision, recall, F1-score, ROC curve, and AUC (Area Under the Curve).
    // [8] Regularization: To prevent overfitting, logistic regression models can be regularized using techniques like L1 regularization (Lasso) or L2 regularization (Ridge).
    // [9] Multinomial Logistic Regression: While binary logistic regression deals with two classes, multinomial logistic regression extends the technique to handle more than two classes.

// Logistic regression is widely used in various fields, including healthcare (disease prediction), marketing (customer churn analysis), finance (credit risk modeling), and many 
// other applications where binary classification or probability estimation is essential. It's a fundamental tool in the machine learning and statistical analysis toolkit.

//-----------------------------------------------------------------------------{
// Utils
//-----------------------------------------------------------------------------{

export dot(series float v, series float w, series int p) =>  // dot product
    // \sum_{i = 1}^{p} v \dot w
    // @function dot : Returns the inner product of two vectors
    // @params: <float v> : First vector
    // @params: <float w> : Secod vector
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : The inner product of two vectors
    math.sum(v * w, p)

export minimax(series float value, series int p, series float min,series float max) =>
    // (max - min) * (value - low) / (high - minimum) + minimum
    // @function Price normalization : Returns normalization value of the prediction
    // @params <int value> : input value to normalize
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float min> : min value over range of p-values (not probabilities)
    // @params: <float max> : max value over range of p-values (not probabilities)
    // @returns <float float> : minimax value
    hi = ta.highest(value, p)
    lo = ta.lowest(value, p)
    (max - min) * (value - lo) / (hi - lo) + min

export reqularizer(string r, series float w, series int p, series float alpha, series float lambda) =>
    // @function Regularization term : Returns regularization quantity to control overfitting and underfitting
    // @params: <string r> : choice of regularizer e.g L1-norm, l2-norm or L1-L2 (Elastic Net)
    // @params: <float w> : training weight
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float alpha> : Hyperparameter (inner) I
    // @params: <float lambda> : Hyperparameter (outer) II
    // @returns <float float> : L1, L2 or L1-L2 (Elastic Net) regularization
    switch r
        'l1' => lambda * math.abs(w)
        'l2' => lambda * dot(w, w, p)
        'enet' => lambda * dot(alpha * dot(w, w, p), (1 - alpha) * math.abs(w), p)
        'nr' => 0.0 //No regularization


export sigmoid(series float z) => 
    // 1.0 / (1.0 + exp(-z))
    // @function Sigmoid : Returns the output of a sigmoid function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    1.0 / (1.0 + math.exp(-z))

// Cost function of Logistic Regression
export cost_logit(series float Y, series float y_hat, series int p) =>
    // -1.0 / p \sum_{i = 1}^{p} y_i ln y_{hat} + (1 - y_i) ln (1 - y_hat)
    // @function loss function of Logit : Returns the loss function of Logistic regression model
    // @params: <float Y> : input data of the label to classify
    // @params: <float y_hat> :The hypothesis of the classification on Y
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : cost (loss) output
    1.0 / p * dot(dot(Y, math.log(y_hat), p), dot((1.0 - Y), math.log(1 - y_hat), p), p)

//-----------------------------------------------------------------------------{
// Training Logistic Regression
//-----------------------------------------------------------------------------{

//------ Training Regularized Logistic Regression with Elastic Net
export logistic_regression(series float X, series float Y, series int p, series float lr, series float _lambda, series float _alpha, series float epochs, series string r) =>
    // @function Training function : Returns the training loss and hypothesis of the Elastic Net logistic regression model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float _lambda> : Weight parameter fixed on the regularizer e.g 0.1
    // @params: <float _alpha> : Weight parameter fixed on the regularizer e.g 0.5
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for Elastic Net regularized logistic regression
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = sigmoid(dot(X, 0.0, p))  //-- prediction
        loss := cost_logit(Y, y_hat, p) + reqularizer(r, w, p, _alpha, _lambda)
        gradient = 1.0 / p * dot(X, y_hat - Y, p)
        w -= lr * gradient  //-- update weights
        w
    [loss, sigmoid(dot(X, w, p))]  //-- current loss & prediction

//-----------------------------------------------------------------------------{
// Support Vector Machine
//-----------------------------------------------------------------------------{

// Support Vector Machine (SVM) is a machine learning algorithm used for classification and regression tasks. Its primary goal is to find an 
// optimal hyperplane that best separates data points into different classes while maximizing the margin between them. SVMs are effective in 
// handling both linear and non-linear data and have applications in various fields such as image classification, text analysis, and 
// bioinformatics. They rely on the concept of support vectors and can use different kernel functions to transform data for improved separation. 
// SVMs are known for their ability to provide accurate results and their versatility in solving complex problems. However, they can be 
// computationally intensive, and hyperparameter tuning is crucial for their performance.

//In a linear Support Vector Machine (SVM), there are several tunable hyperparameters that can be adjusted to optimize its performance. These 
//hyperparameters include:

    // [1] C (Regularization Parameter): C controls the trade-off between maximizing the margin and minimizing the classification error. A smaller 
    //                                 C value results in a larger margin but may allow some misclassification, while a larger C value aims for 
    //					more precise classification at the cost of a smaller margin. Tuning C is essential for balancing bias and variance.

    // [2] Kernel Choice: In a linear SVM, the choice of kernel is typically "linear" since it operates in a linear feature space. However, some 
    //			  SVM implementations may allow you to specify different kernel functions (e.g., polynomial or RBF) even for linear SVM. 
    //			  Ensuring that the kernel is set to "linear" is important for linear SVM.

    // [3] Loss Function: Some SVM implementations may allow you to choose between different loss functions, such as "hinge loss" or "squared 
    //			hinge loss." These can affect the optimization process and the margin's robustness.

    // [4] Tolerance (tol): The tolerance parameter determines the stopping criterion for the optimization algorithm. It controls the threshold 
    //			    for convergence. You can adjust it to influence the speed of convergence and potentially trade off computational efficiency for precision.

    // [5] Class Weights (class_weight): If your dataset is imbalanced, you can assign different weights to the classes to influence the SVM's 		
    //					 behavior. This can help ensure that the SVM pays more attention to the minority class during training.

    // [6] Max Iterations (max_iter): This hyperparameter specifies the maximum number of iterations the SVM optimization algorithm is allowed to 
    //				      run. It can be adjusted to control the training time, especially for large datasets or when convergence is 
    //                                slow.
    
//-----------------------------------------------------------------------------{
// Utils
//-----------------------------------------------------------------------------{

//------ Margin 
export margin(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the SVM margin or hypothesis
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : Margin
    Y * (dot(X, w, p) + b)

//--SVM hinge loss
export hinge(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the the SVM hinge loss
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : hinge loss
    math.max(0, 1 - margin(X, Y, w, b, p))

//----- Cost function
export cost_svm(series float X, series float Y, series float w, series float b, series int p) =>
    // @function Training function : Returns the training loss and hypothesis of SVM
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params: <float w> : The training weight
    // @params <float b> : The model bias. usually initialized to 0.0
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : cost (total loss) output
    float C = 0.01
    1.0 / p * C * math.sum(hinge(X, Y, w, b, p), p) // Regularizer <- 1 / p * 0.5 * dot(w, w, p)

//-- Output axon
export svm_axon(series float z) =>
    // @function Sigmoid : Returns the output of the SVM function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    math.sign(z)

//-----------------------------------------------------------------------------{
// Training an SVM
//-----------------------------------------------------------------------------{

//--- Training Linear SVM
export linear_SVM(series float X, series float Y, series int p, series float lr, series float C, series float _lambda, series float _alpha, series float epochs, series string r) =>
    // w -> weight; loss -> SVM loss; b -> bias weight C -> relaxation term
    // @function Training function : Returns the training loss and hypothesis of linear Support Vector Machine (SVM)
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float lr> : The learning rate
    // @params: <float C> : Weight parameter fixed and tunable hyperparameter e.g 0.1
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output for SVM
    float w = 0.0
    float loss = 0.0
    float b = 0.0
    for i = 1 to epochs
        y_hat = margin(X, Y, 0.0, 0.0, p)
        if y_hat < 1
            loss := cost_svm(X, Y, w, b, p) + reqularizer(r, w, p, _alpha, _lambda) //you can set \lambda <- 0.5
            gradient = 1.0 / p * (b - C * dot(Y, X, p))
            b := 1.0 / p * (b - lr * C * math.sum(Y, p))
            w -= lr * gradient
            w
    [loss, svm_axon(dot(X, w, p) + b)]

//-----------------------------------------------------------------------------{
// Perceptron
//-----------------------------------------------------------------------------{

// The Perceptron is a fundamental concept in machine learning and artificial intelligence. It is a binary linear classifier that models a 
// decision boundary to separate data into two classes. The Perceptron algorithm is based on the workings of a biological neuron, with inputs 
// being weighted and summed to produce an output. While simple, Perceptrons can be effective for linearly separable data but have limitations 
// when dealing with complex problems. They serve as a building block for more advanced neural network architectures like multi-layer perceptrons 
// (MLPs).

//-----------------------------------------------------------------------------{
// Utils
//-----------------------------------------------------------------------------{

//------------ Prediction
export tanh(series float X, series float w, series int p) =>
    // @function tanh : Returns the hyperbolic tangent (tanh) of a vector
    // @params: <float X> : Input predictor data
    // @params: <float w> : training weight
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @returns <float float> : Returns the hyperbolic tangent (tanh)
    (math.exp(dot(X, w, p)) - math.exp(-dot(X, w, p))) / (math.exp(dot(X, w, p)) + math.exp(-dot(X, w, p)))

//------------- Cost function
export cost_perceptron(series float Y, series float y_hat) =>
    // @function Training cost function : Returns the training loss of a perceptron
    // @params: <float Y> : Input data of the label to classify
    // @params: <float y_hat> :The hypothesis of the classification on Y
    // @returns <float float> : cost (loss) output for Perceptron
    Y - y_hat

//------------- Activation function
export activation_perceptron(series float z) =>
    // @function Perceptron activation : Returns the output of a perceptron function
    // @params: <float z> : vector, usually the hypothesis
    // @returns <float float> : output of a sigmoid function
    result = sign(z) > 0 ? 1 : 0
    result

//-----------------------------------------------------------------------------{
// Training a Perceptron
//-----------------------------------------------------------------------------{

//------- Training a perceptron
export perceptron(series float X, series float Y, series int p, series float _lambda, series float _alpha, series float epochs, series string r) =>
    // @function Training function : Returns the training loss and hypothesis of Perceptron model
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params: <float Y> : Input data of the label to classify
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params: <float epochs> : The number of training steps
    // @returns <float float> : cost (loss) output
    float w = 0.0
    float loss = 0.0
    for i = 1 to epochs
        y_hat = tanh(X, w, p)
        if Y != y_hat
            loss := cost_perceptron(Y, y_hat) + reqularizer(r, w, p, _alpha, _lambda)
            w += 0.1 * dot(loss, X, p)
            w
    [loss, activation_perceptron(dot(X, w, p))]

//-----------------------------------------------------------------------------{
// k-Nearest Neighbors (k-NN)
//-----------------------------------------------------------------------------{

// kNN is based on the idea that data points with similar features tend to belong to the same class or have similar values. 
// It makes predictions by considering the -nearest data points (neighbors) in the training dataset to the data point you want to classify (for classification) or 
// predict (for regression). It is often referred to as a lazy learner since it does not explicitly build a model during training. 
// Instead, it stores the training data and makes predictions at runtime based on the nearest neighbors. 
// In most cases, the choice of distance metric (such as Euclidean distance, Manhattan distance or Lorentzian distance) determines the 
// performance of the algorithm before tuning the hyperparameter .

export D(series float x1, series float x2, series string dt) =>
    // @function Distance measure to compute the nearest neighbors
    // @params: <float x1> : Input data to compare with next closest point [x2]
    // @params: <float x2> : Input data to compare with next closest point [x1]
    // @params: <string dt> : distance measure type for instance, Euclidean, Lorentzian, Canberra, Minkowski etc
    switch dt
        'euclidean' => math.sqrt(math.pow(x1 - x2, 2))
        'lorentzian' => math.log(math.sqrt(math.pow(x1 - x2, 2)))
        'canberra' => math.abs(x1 - x2) / ((math.abs(x1) + math.abs(x2) + 1e-8))
        'minkowski' => math.pow(math.pow(math.abs(x1 - x2), 2), 1/3) //the power (1/p) is fixed (@ 1/3) here but it is actually a variable.
        'absolute' => math.abs(x1 - x2)

export kNN(series float X, series int p, series int k, series string dt) => 
    // @params: <float X> : Input predictor required to classify a given label Y
    // @params <int p> : lookback value is the number of historical data to consider when computing the dot product
    // @params <int k> : number of k nearest neighbours
    // @params: <string dt> : distance measure type for instance, Euclidean, Lorentzian, Canberra, Minkowski etc
    // @returns <float float> : prediction and market direction
    nearest_neighbors = array.new_float(0)
    distances         = array.new_float(0)
    for i = 0 to p-1
        float d = D(X[i], X[i+1], dt)
    	array.push(distances, d)
    	int size = array.size(distances)
    	//---
        float new_neighbor = d < array.min(distances, size > k ? k : 0) ? X[i+1] : X[i]
        array.push(nearest_neighbors, new_neighbor)
    //--prediction on all datatset
    float prediction = array.avg(nearest_neighbors)
    int   direction  = prediction < X ? 1 : prediction > X ? -1 : 0
    [prediction, direction]



// Example
reg = 'nr'
p = 50
nm = 5
alpha = 0.1
lambda = 0.5
//-logit
[loss_lgit, y_hat_lgit] = logistic_regression(close, math.log(hl2), p, 1e-3, lambda, alpha, 1000, reg)
y_hat_scaled_lgit = minimax(y_hat_lgit, nm, ta.lowest(close, nm), ta.highest(close, nm))
//-SVM
[loss_svm, y_hat_svm] = linear_SVM(close, hl2, p, 1e-3, 0.1, lambda, alpha, 1000, reg)
y_hat_scaled_svm = minimax(y_hat_svm, nm, ta.lowest(close, nm), ta.highest(close, nm))
//-Perceptron
[loss_p, y_hat_p] = perceptron(close, hl2, p, lambda, alpha, 1000, reg)
y_hat_scaled_p = minimax(y_hat_p, nm, ta.lowest(close, nm), ta.highest(close, nm))
//-Render plots
plot(y_hat_scaled_lgit, color = color.white, title = "Logit")
plot(y_hat_scaled_svm, color = color.red, title = "SVM")
plot(y_hat_scaled_p, color = color.blue, title = "Perceptron")


